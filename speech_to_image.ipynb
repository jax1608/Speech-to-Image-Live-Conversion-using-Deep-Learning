{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd86225-b68b-41ab-a148-03d8fbb54040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba746d31-49b9-477f-9f38-eca391de22e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d10deda-69a3-4a37-8021-0d1083aca0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo apt-get install libportaudio2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9f5e8-134f-4515-9e74-756a82056ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"Audio\\download.mp3\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e353d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5143c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path):\n",
    "    # Load the audio file\n",
    "    waveform, sample_rate = torchaudio.load(\"Audio\\Hanumankind.mp3\")\n",
    "    # Convert to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0)\n",
    "    # Normalize audio\n",
    "    waveform = waveform / waveform.abs().max()\n",
    "    return waveform, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from jiwer import wer\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result1 = model.transcribe(\"Audio/download.mp3\")\n",
    "prediction1 = result1[\"text\"]\n",
    "print(prediction1)\n",
    "hypothesis1 = \"Hello my name is Adi and I am here to tell you about Whisper API. Open-Ai WHISPER is an amazing thing and it is something that you will definitely use if you can understand how powerful it is and how to use it.\"\n",
    "error = wer(prediction1, hypothesis1)\n",
    "print(error1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdeb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torchaudio\n",
    "from fuzzywuzzy import process\n",
    "import numpy as np\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Preprocess Audio\n",
    "def preprocess_audio(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0)\n",
    "    waveform = waveform / waveform.abs().max()\n",
    "    return waveform.numpy()\n",
    "\n",
    "# Transcribe Audio\n",
    "def transcribe_audio(model, file_path):\n",
    "    result = model.transcribe(file_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# Song Matching\n",
    "song_database = {\n",
    "    \"Shape of You\": \"lyrics of shape of you\",\n",
    "    \"Blinding Lights\": \"lyrics of blinding lights\",\n",
    "    \"Bohemian Rhapsody\": \"lyrics of bohemian rhapsody\",\n",
    "}\n",
    "\n",
    "def match_song_title(transcription, song_database):\n",
    "    best_match = process.extractOne(transcription, song_database.values())\n",
    "    for title, lyrics in song_database.items():\n",
    "        if lyrics == best_match[0]:\n",
    "            return title\n",
    "\n",
    "# Identify Song\n",
    "def identify_song(file_path):\n",
    "    transcription = transcribe_audio(model, file_path)\n",
    "    song_title = match_song_title(transcription, song_database)\n",
    "    return song_title\n",
    "\n",
    "# Example usage\n",
    "audio_file = \"Audio/Preview.mp3\"\n",
    "song_title = identify_song(audio_file)\n",
    "print(f\"The song title is: {song_title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2dc8f-6c3a-464b-86cd-0bdf83ccc4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Function to record audio\n",
    "def record_audio(duration=5, samplerate=16000):\n",
    "    print(\"Recording...\")\n",
    "    audio = sd.rec(int(samplerate * duration), samplerate=samplerate, channels=1, dtype='float32')\n",
    "    sd.wait()\n",
    "    print(\"Recording finished.\")\n",
    "    return audio.flatten()\n",
    "\n",
    "# Function to transcribe audio using Whisper\n",
    "def transcribe_audio(audio):\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "    options = whisper.DecodingOptions(language=\"en\")\n",
    "    result = model.decode(mel, options)\n",
    "    return result.text\n",
    "\n",
    "try:\n",
    "    duration = 5  # Set recording duration\n",
    "    audio = record_audio(duration)\n",
    "    text = transcribe_audio(audio)\n",
    "    text = text.lower()\n",
    "    print(f\"Recognized: {text}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1402bb9c-513a-4c6f-826e-ec2973ac1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Load various NLP models\n",
    "summarizer = pipeline(\"summarization\")\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "ner_tagger = pipeline(\"ner\")\n",
    "\n",
    "# Function to record audio\n",
    "def record_audio(duration=5, samplerate=16000):\n",
    "    print(\"Recording...\")\n",
    "    audio = sd.rec(int(samplerate * duration), samplerate=samplerate, channels=1, dtype='float32')\n",
    "    sd.wait()\n",
    "    print(\"Recording finished.\")\n",
    "    return audio.flatten()\n",
    "\n",
    "# Function to transcribe audio using Whisper\n",
    "def transcribe_audio(audio):\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "    options = whisper.DecodingOptions(language=\"en\")\n",
    "    result = model.decode(mel, options)\n",
    "    return result.text\n",
    "\n",
    "try:\n",
    "    duration = 5  # Set recording duration\n",
    "    audio = record_audio(duration)\n",
    "    text = transcribe_audio(audio)\n",
    "    text = text.lower()\n",
    "    print(f\"Recognized: {text}\")\n",
    "    \n",
    "    # NLP Processing: Summarization\n",
    "    summary = summarizer(text, max_length=50, min_length=25, do_sample=False)[0]['summary_text']\n",
    "    print(f\"Summary: {summary}\")\n",
    "    \n",
    "    # NLP Processing: Sentiment Analysis\n",
    "    sentiment = sentiment_analyzer(text)[0]\n",
    "    print(f\"Sentiment: {sentiment['label']} with score {sentiment['score']}\")\n",
    "    \n",
    "    # NLP Processing: Named Entity Recognition\n",
    "    entities = ner_tagger(text)\n",
    "    print(\"Named Entities:\")\n",
    "    for entity in entities:\n",
    "        print(f\"{entity['word']} ({entity['entity']})\")\n",
    "    \n",
    "    # Print keywords (simple extraction by splitting text)\n",
    "    keywords = set(text.split())\n",
    "    print(\"Keywords:\")\n",
    "    for keyword in keywords:\n",
    "        print(keyword)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3250c3b1-ec30-4a6c-8e53-2a6ef96ffb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load the stable diffusion model\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id).to(\"cuda\")\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"A horse\"\n",
    "\n",
    "# Generate the image\n",
    "with torch.autocast(\"cuda\"):\n",
    "    image = pipe(prompt, guidance_scale=7.5).images[0]\n",
    "\n",
    "# Save the image\n",
    "image.save(\"generated_image.png\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922044e5-6574-49c7-a795-9707ad8d7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e4a14e-5670-44c4-8007-5d38279777bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "transcription = \"A horse\"\n",
    "pipe = load_stable_diffusion()\n",
    "image = generate_image(transcription, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce92b0ba-8f81-4a1f-8bf4-15369ab8b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "# Load the Stable Diffusion pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")  \n",
    "prompt = \"a boy in moon\"\n",
    "with torch.autocast(\"cuda\"):\n",
    "    image = pipe(prompt).images[0]  \n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1b68b-fd8a-4851-9f42-51dac7fc503e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
